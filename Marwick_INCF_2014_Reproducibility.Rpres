

A primer on reproducible research: Basic principles and common tools
========================================================
author: Ben Marwick, University of Washington
date: December 2014
transition: none


Overview
======================================================== 
<div><span style="font-size:1.8em; line-height:1em">
<ul>
<li> Definitions, motives, spectrum    
<li> Current practices     
<li> What I'm doing to improve reproducibility     
<li> What can be done to improve reproducibility?  
</ul>
</span></div>

Definitions are in flux
========================================================

![alt text](figures/definitions.png)

<div><span style="font-size:1.5em; line-height:1em">
Computational - Statistical - Emprirical
</span></div>

<div><span style="font-size:0.5em; line-height:0.5em">
Stodden, V., et al. 2013. "Setting the default to reproducible." computational science research. SIAM News 46: 4-6.
</span></div>


Motivations: Claerbout's principle
========================================================

>"An article about a computational result is advertising, not scholarship. The actual scholarship is the full software environment, code and data, that produced the result." <small><small>- Claerbout and Karrenbach, Proceedings of the 62nd Annual International Meeting of the Society of Exploration Geophysics. 1992</small></small>


Claerbout's principle in detail
========================================================

>"When we publish articles containing figures which were generated by computer, we also publish the complete software environment which generates the figures" <small><small>- Buckheit & Donoho, Wavelab and Reproducible Research, 1995.</small></small>

>"The scholarship does not only consist of theorems and proofs but also (and perhaps even more important) of data, computer code and a runtime environment which provides readers with the possibility to reproduce all tables and figures in an article." <small><small>- Hothorn et al., 2009 Case studies in reproducibility</small></small>


Benefits are straightforward
=======================================================
- **Verification & Reliability**: Easier to find and fix bugs. The results you produce today will be the same results you will produce tomorrow.
- **Transparency**: Leads to broader impact, increased citation count, improved institutional memory
- **Efficiency**: Reuse allows for de-duplication of effort. Payoff in the (not so) long run
- **Flexibility**: When you don’t 'point-and-click' you gain many new analytic options.


But the limitations are substantial
=======================================================

**Technical**
- Classified/sensitive/big data 
- Software licensing issues
- Competition
- Neither necessary nor sufficient for correctness (but essential for dispute resolution)

***

**Cultural & personal**
- Reproducibility is not rewarded
- No-one expects or requires reproducibility 
- No uniform standards of reproducibility, so no established user base
- Embarassment

Our work exists on a spectrum of reproducibility
=======================================================
![alt text](figures/peng-spectrum.jpg)
<small><small>Peng 2011, Science 334(6060) pp. 1226-1227</small></small>

Goal is to expose the reader to more of the research workflow
=======================================================
![alt text](figures/peng-pipeline.jpg)
<small><small><small>http://www.stodden.net/AMP2011/slides/pengslides.pdf</small></small></small>

Current practices in many disciplines
========================================================
- Enter data in Excel
- Use Excel for data cleaning & descriptive statistics
- Import data into SPSS/SAS/Stata for further analysis
- Use point-and-click options to run statistical analyses
- Copy & paste output to Word document, repeatedly

***

![alt text](figures/beaker.jpg)


========================================================
**Click-trails compromise clarity**

- Lots of human effort for tedious & time-wasting tasks
- Error-prone due to manual & ad hoc data handling (column and row offsets are common)
- Difficult to record -  hard to reconstruct a 'click history'
- Tiny changes in data or method require extensive reworking efforts

***

**Scripted analyses support scientific integrity**

- Plain text files will be readable for a long time
- Improved transparency, automation, maintanability, accessibility, portability, efficiency, communicability of process (what more could we want?)
- <font color="red">But there's a steep learning curve</font> 



========================================================
<div><span style="font-size:4em; line-height:1em">
What am I doing to encourage sharing?
</div></span>

========================================================
Using literate statistical programming

Using an open source programming language

Using an open document formatting language

Using version control 

Using dynamic documents

***
             
-------------- ----------- --------------
<img src="figures/r-project.jpg" width="120">   <img src="figures/markdown.png" width="120"> <img src="figures/git.png" width="170">

![alt text](figures/workflow-wide.png)
<div align="center"><img src="figures/knitr.png" alt="alt text" width="200"></div>


Using convenient tools, services & support
========================================================
`

`

![alt text](figures/docker.png)
![alt text](figures/rstudio.png)
![alt text](figures/zenodo.png)


***

<img src="figures/escience.jpg" alt="alt text" width="450">
![alt text](figures/ropensci.png)
<img src="figures/swc.png" alt="alt text" width="450">
![alt text](figures/so.png)



========================================================
<div><span style="font-size:4em; line-height:1em">
What can be done to improve code & data sharing?
</div></span>


========================================================
<img src="figures/VictoriaStoddenIASSISTJune2010-reasons-to.png" alt="alt text" width="800">
<small><small>Stodden (IASSIST 2010) sampled American academics registered at the Machine Learning conference NIPS (134 responses from 593 requests (23%). Red = communitarian norms, Blue = private incentives</small></small>


========================================================
<img src="figures/VictoriaStoddenIASSISTJune2010-reasons.png" alt="alt text" width="800">
<small><small>Stodden (IASSIST 2010) sampled American academics registered at the Machine Learning conference NIPS (134 responses from 593 requests (23%). Red = communitarian norms, Blue = private incentives</small></small>

Speed up culture change with incremental steps
========================================================
- **Promote culture change** through positive attribution 
- Implement mechanisms to indicate & encourage **degrees of compliance** (ie. clear definitions for different levels of reproducibility), cf. Stodden's:
 - 'Reproducible': compendium of text-code-data online
 - 'Reproduced': compendium available and independently reproduced 
 - 'Semi-Reproducible': when the full compendium is not released
 - 'Semi-Reproduced': independent reproduction with other data
 
 
Promote existing standards to normalise reproducible research
========================================================
- Schwab et al.: ER (Easily reproducible), CR (Conditionally reproducible), NR (Not reproducible)
- _Biostatistics_ opt-in **kite-marking of articles** (Peng 2009): D (data), C (code), R (both)
- **Reproducible Research Standard** (Stodden 2009): we should  
 - Release the full compendium on the internet
 - License media such as text, figures, tables with Creative Commons Attribution license (CC-BY) 
 - License code with one of Apache 2.0, MIT, LGPL, BSD, etc.
 - License "selection and arrangement" of data with CC0 or CC-BY


Use the Center for Open Science's badges
========================================================
<img src="figures/badges.png" alt="alt text" width="800">


An **incentive** to share data and code by acknowledging open practices with badges in publications. Currently used by _Psychological Science_


Integrate these values into everyday tasks
========================================================
- **Train students** by putting homework, assignments & dissertations on the reproducible research spectrum
- **Publish examples** of reproducible research in our field
- **Request** code & data when reviewing
- **Submit to & review for journals** that support reproducible research
- Critically review & audit **data management plans** in grant proposals
- Consider reproducibility wherever possible in **hiring, promotion & reference letters**.

Thanks.
========================================================
>"Abandoning the habit of secrecy in favor of process transparency and peer review was the crucial step by which alchemy became chemistry."
---
<small><small>-Raymond, E. S., 2004, The art of UNIX programming: Addison-Wesley.</small></small>

Colophon
========================================================
Presentation written in [Markdown](http://daringfireball.net/projects/markdown/) ([R Presentation](http://www.rstudio.com/ide/docs/presentations/overview))

Compiled into HTML5 using [RStudio](http://www.rstudio.com/ide/)

Source code hosting: https://github.com/benmarwick/INCF-Sharing--A-primer-on-reproducible-research


ORCID: http://orcid.org/0000-0001-7879-4531

Licensing: 

* Presentation: [CC-BY-3.0 ](http://creativecommons.org/licenses/by/3.0/us/)

* Source code: [MIT](http://opensource.org/licenses/MIT) 


```{r, echo=FALSE, eval = FALSE}

References

[Barnes, N. Publish your computer code: it is good enough. Nature. 467, (2010), 753.](http://www.nature.com/news/2010/101013/full/467753a.html) [pdf](http://www.nature.com/news/2010/101013/pdf/467753a.pdf)

Baumer, B. et al. R Markdown: Integrating a Reproducible Analysis Tool in Introductory Statistics. (2014) Technology Innovations in Statistics Education.  8(1). 1-29. [pdf](http://escholarship.org/uc/item/90b2f5xh.pdf)

[Birney, E. et al. Prepublication data sharing. Nature. 461, (2009), 168-170.](http://www.nature.com/nature/journal/v461/n7261/full/461168a.html) [pdf](http://www.nature.com/nature/journal/v461/n7261/pdf/461168a.pdf)

Buckheit, J.B. and Donoho, D.L. Wavelab and reproducible research. (1995). [pdf](http://www-stat.stanford.edu/~wavelab/Wavelab_850/wavelab.pdf)

Drummond, C. Reproducible Research: a Dissenting Opinion. (2012), 1-10. [pdf](http://cogprints.org/8675/1/ReproducibleResearch.pdf)

Freire, J. et al. Computational reproducibility: state-of-the-art, challenges, and database research opportunities. SIGMOD2012 (2012), 593-596. [pdf](http://bigdata.poly.edu/~juliana/pub/freire-sigmod2012.pdf)

Gandrud C 2013 Reproducible Research with R and RStudio. CRC Press Florida. [link](http://christophergandrud.github.io/RepResR-RStudio/)

Gentleman, R. and Temple Lang, D. (2007). Statistical analyses and reproducible research. Journal of Computational and Graphical Statistics 16, 1–23. [pdf](http://www.extemporaneousb.com/R.berkeley/resources/StatisticalAnalysesAndReproducibility.pdf)

Hothorn T and Leisch F 2011. Case studies in reproducibility. Briefings in Bioinformatics 12(3), 288-300. [pdf](http://www.statistik.tu-dortmund.de/fileadmin/user_upload/Lehrstuehle/Genetik/KS1112/Hothorn2011.pdf)

Ioannidis, J.P. a et al. Repeatability of published microarray gene expression analyses. Nature genetics. 41, (2009), 149-55. [pdf](http://www.csc.mrc.ac.uk/d/Petretto/people_htm_files/4.pdf)

King, G. Replication, Replication. PS: Political Science and Politics. (1995).[pdf](http://gking.harvard.edu/files/gking/files/replication.pdf)

Leisch F, Eugster M and Hothorn T 2011. Executable Papers for the R Community: The R2 Platform for Reproducible Research. Procedia Computer Science 4(0), 618-626. [pdf](http://www.sciencedirect.com/science/article/pii/S1877050911001232/pdf?md5=fbaaaf6f08354bd22db760c9bd113ee2&pid=1-s2.0-S1877050911001232-main.pdf)

LeVeque, R.J. Python tools for reproducible research on hyperbolic problems. Computing in Science & Engineering. (2009),19-27. [pdf](http://citeseerx.ist.psu.edu/viewdoc/download?rep=rep1&type=pdf&doi=10.1.1.219.5569)

LeVeque, R.J. Wave propagation software, computational science, and reproducible research. Proceedings of the International Congress of Mathematicians (Madrid, Spain, 2006), 1-27. [pdf](http://www.mathunion.org/ICM/ICM2006.3/Main/icm2006.3.1227.1254.ocr.pdf)

LeVeque, R, Stodden, V., & Mitchell, I. (2012). Reproducible Research for Scientific Computing: Tools and Strategies for Changing the Culture. Computing in Science and Engineering, 14(4), 13–17 [pdf](http://www.stanford.edu/~vcs/papers/CiSE2012-LMS.pdf)

McCullough, B.D. Got Replicability? The Journal of Money, Credit and Banking Archive. Econ Journal Watch. 4, (2007),326-337 [pdf](http://econjwatch.org/file_download/170/2007-09-mccullough-econ_practice.pdf)

McCullough, B.D. Do economics journal archives promote replicable research?. Economics Journal Archives. (2008). [pdf](http://www.pages.drexel.edu/~bdm25/cje.pdf)

Manolescu, I. et al. Repeatability & Workability Evaluation of SIGMOD 2009. SIGMOD 2009 (2009), 2-4. [pdf](http://hal.archives-ouvertes.fr/docs/00/43/34/58/PDF/SIG2009RW-rev2.pdf)

McCullough BD, Heiser DA (2008). “On the Accuracy of Statistical Procedures in Microsoft Excel 2007.” Computational Statistics & Data Analysis, 52, 4570–4578. [pdf](http://www.pucrs.br/famat/viali/tic_literatura/artigos/planilhas/McHe08.pdf)

Merali, Z. Error: Why scientific programming does not compute. Nature. (2010), 6-8. [pdf](http://www.nature.com/news/2010/101013/pdf/467775a.pdf)

Morin, A. et al. Shining light into black boxes. Science. 336, (2012), 159-160. [pdf](http://www.salilab.org/pdf/Morin_Science_2012.pdf)

[Peng, R.D. Reproducible research and Biostatistics. Biostatistics (Oxford, England). 10, (2009), 405-408.](http://biostatistics.oxfordjournals.org/content/10/3/405.full)

Price, K. Anything You Can Do, I Can Do Better (No You Can't)... Computer Vision, Graphics, and Image Processing. (1986), 387-391. [pdf](http://pdf.thepdfportal.com/PDFFiles/121612.pdf)

[Piwowar, H. a et al. Sharing detailed research data is associated with increased citation rate. PloS one. 2, (2007), 308.](http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0000308#pone-0000308-g002)
                                                                                                                                                                                                                                                      Piwowar HA, Vision TJ. (2013) Data reuse and the open data citation advantage. PeerJ 1:e175 http://dx.doi.org/10.7717/peerj.175

Quirk, J. Computational Science "Same Old Silence, Same Old Mistakes" "Something More Is Needed..." Adaptive Mesh Reenement-Theory and Applications. (2005), 4-28. [link](http://link.springer.com/chapter/10.1007/3-540-27039-6_1)

Rossini, Anthony and Leisch, Friedrich, "Literate Statistical Practice" (March 2003). UW Biostatistics Working Paper Series. Working Paper 194. [link](http://biostats.bepress.com/uwbiostat/paper194)

Sandve GK, Nekrutenko A, Taylor J, Hovig E (2013) Ten Simple Rules for Reproducible Computational Research. PLoS Comput Biol 9(10): e1003285. doi:10.1371/journal.pcbi.1003285 [link](http://www.ploscompbiol.org/article/info%3Adoi%2F10.1371%2Fjournal.pcbi.1003285)

[Savage, C.J. and Vickers, A.J. Empirical study of data sharing by authors publishing in PLoS journals. PloS one. 4, (2009),7078.](http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0007078#pone-0007078-g001)

Schofield, P.N. et al. Post-publication sharing of data and tools. Nature. 461, (2009), 171-173. [pdf](http://www.nature.com/nature/journal/v461/n7261/pdf/461171a.pdf)

Stodden, V. The Legal Framework for Reproducible Scientific Research: Licensing and Copyright. Computing in Science &Engineering. 11, (2009), 35-40. [pdf](http://academiccommons.columbia.edu/download/fedora_content/download/ac:140154/CONTENT/04720221.pdf)

Stodden, “Trust Your Science? Open Your Data and Code,” Amstat News, 1 July 2011; http://magazine.amstat.org/blog/2011/07/01/trust-your-science/

Stodden V, Guo P, Ma Z (2013) Toward Reproducible Computational Research: An Empirical Analysis of Data and Code Policy Adoption by Journals. PLoS ONE 8(6): e67111. doi:10.1371/journal.pone.0067111 [link](http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0067111)

Stodden V  2010 The Scientific Method in Practice: Reproducibility in the Computational Sciences. MIT Sloan School Working Paper 4773-10. [link](http://ssrn.com/abstract=1550193)

Vandewalle, P. et al. Reproducible research in signal processing - What, why, and how. IEEE Signal Processing Magazine.26, (2009), 37-47. [pdf](http://rr.epfl.ch/17/1/VandewalleKV09.pdf)

[Wilson, G. et al. Best Practices for Scientific Computing. 1-6.](http://www.plosbiology.org/article/info%3Adoi%2F10.1371%2Fjournal.pbio.1001745)

Xie Y 2013 Dynamic Documents with R and knitr. CRC Press Florida [link](http://www.crcpress.com/product/isbn/9781482203530) [Repo](https://github.com/yihui/knitr-book/)




## Blogs posts and websites on reproducible research

http://blog.stodden.net/2013/04/19/what-the-reinhart-rogoff-debacle-really-shows-verifying-empirical-results-needs-to-be-routine/

http://kbroman.github.io/Tools4RR/

http://rpubs.com/bbolker/3153

http://sepwww.stanford.edu/data/media/public/sep//jon/repropreface.html

http://blog.revolutionanalytics.com/2010/10/a-workflow-for-r.html

http://www.stanford.edu/~vcs/AAAS2011/

http://wiki.stodden.net/Best_Practices_for_Researchers_Publishing_Computational_Results

http://www.reproducibleresearch.net/index.php/RR_links

http://www.nature.com/nature/focus/reproducibility/

http://fperez.org/py4science/git.html

http://ivory.idyll.org/blog/replication-i.html

http://www.mendeley.com/groups/1142301/reproducible-research/

http://biostat.mc.vanderbilt.edu/wiki/Main/StatReport

http://reproducibleresearch.net/index.php/How_to

http://www.executablepapers.com/index.html

http://tomwallis.info/category/reproducible-research/

http://simplystatistics.org/2013/08/21/treading-a-new-path-for-reproducible-research-part-1/

http://scienceinthesands.blogspot.co.uk/search/label/reproduciblie%20research

http://scienceinthesands.blogspot.co.uk/2012/08/7-habits-of-open-scientist-2.html

http://scitation.aip.org/content/aip/journal/cise/11/1/10.1109/MCSE.2009.19

http://www.reproducibility.org/RSF/book/rsf/scons/paper_html/node2.html

http://ajrichards.bitbucket.org/lpEdit/ReproducibleResearch.html

http://kieranhealy.org/blog/archives/2014/01/23/plain-text/

http://nicercode.github.io/git/

http://nicercode.github.io/blog/2013-04-05-projects/

http://ivory.idyll.org/blog/tag/reproducibility.html

http://yihui.name/en/tags/ReproducibleResearch

http://www.rstudio.com/ide/download/preview

https://github.com/rstudio/rmarkdown


# Web applications, services & organisations 

http://recomputation.org/

http://sciencecodemanifesto.org/

http://researchcompendia.org/

https://collage.elsevier.com/

http://www.runmycode.org/home/?/CompanionSite/

## Software cited in the presentation (in order of appearance)

# http://www.r-project.org/
# http://cran.r-project.org/web/packages/roxygen2/index.html
#   http://www.rstudio.com/ide/docs/packages/documentation
#   http://adv-r.had.co.nz/Documenting-functions.html
# http://rcharts.io/
#   http://ramnathv.github.io/rCharts/
#   https://github.com/ramnathv/rCharts
# http://ipython.org/
# https://github.com/att/rcloud
#   http://www.research.att.com/articles/featured_stories/2013_09/201309_SandR.html?fbid=RljYLCmQyyR
# http://daringfireball.net/projects/markdown/
# http://rmarkdown.rstudio.com/
# http://yihui.name/knitr/
#   http://cran.r-project.org/web/packages/knitr/index.html
# http://johnmacfarlane.net/pandoc/
# http://git-scm.com/
#   https://github.com/
#   https://bitbucket.org/
# http://www.rstudio.com/ide/
# http://figshare.com/
# http://datadryad.org/
#

```

